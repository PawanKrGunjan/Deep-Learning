{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-01T17:17:34.865719Z",
     "iopub.status.busy": "2023-04-01T17:17:34.865453Z",
     "iopub.status.idle": "2023-04-01T17:17:46.262005Z",
     "shell.execute_reply": "2023-04-01T17:17:46.260924Z",
     "shell.execute_reply.started": "2023-04-01T17:17:34.865690Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install numba\n",
    "#!pip install scikit-learn\n",
    "#!pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-01T17:17:46.263936Z",
     "iopub.status.busy": "2023-04-01T17:17:46.263592Z",
     "iopub.status.idle": "2023-04-01T17:17:46.275910Z",
     "shell.execute_reply": "2023-04-01T17:17:46.275138Z",
     "shell.execute_reply.started": "2023-04-01T17:17:46.263906Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numba'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m njit\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sqrt, sin, cos, pi, zeros\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrandom\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m randn, rand, uniform, normal\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numba'"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numba import njit\n",
    "from numpy import sqrt, sin, cos, pi, zeros\n",
    "from numpy.random import randn, rand, uniform, normal\n",
    "from scipy.linalg import hadamard\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, LSTM, Dropout, RepeatVector, TimeDistributed, Embedding, Reshape, Dot, Concatenate\n",
    "from tensorflow.keras.layers import GRU, SpatialDropout1D, Conv1D, GlobalMaxPooling1D,Multiply, Lambda, Softmax, Flatten, BatchNormalization, Bidirectional, dot, concatenate\n",
    "from tensorflow.keras.layers import AdditiveAttention, Attention\n",
    "from tensorflow.keras.activations import relu\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.metrics import MeanSquaredError\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "print(\"Tensorflow version \" + tf.__version__)\n",
    "AUTO = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-01T17:17:46.277184Z",
     "iopub.status.busy": "2023-04-01T17:17:46.276932Z",
     "iopub.status.idle": "2023-04-01T17:17:46.292362Z",
     "shell.execute_reply": "2023-04-01T17:17:46.291544Z",
     "shell.execute_reply.started": "2023-04-01T17:17:46.277162Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPLICAS:  1\n"
     ]
    }
   ],
   "source": [
    "# Detect hardware, return appropriate distribution strategy\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-01T17:17:46.293774Z",
     "iopub.status.busy": "2023-04-01T17:17:46.293475Z",
     "iopub.status.idle": "2023-04-01T17:17:46.303791Z",
     "shell.execute_reply": "2023-04-01T17:17:46.302928Z",
     "shell.execute_reply.started": "2023-04-01T17:17:46.293739Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accelerated Linear Algebra enabled\n"
     ]
    }
   ],
   "source": [
    "MIXED_PRECISION = False\n",
    "XLA_ACCELERATE = True\n",
    "\n",
    "if MIXED_PRECISION:\n",
    "    from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "    if tpu: policy = tf.keras.mixed_precision.experimental.Policy('mixed_bfloat16')\n",
    "    else: policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n",
    "    mixed_precision.set_policy(policy)\n",
    "    print('Mixed precision enabled')\n",
    "\n",
    "if XLA_ACCELERATE:\n",
    "    tf.config.optimizer.set_jit(True)\n",
    "    print('Accelerated Linear Algebra enabled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_kg_hide-output": false,
    "execution": {
     "iopub.execute_input": "2023-04-01T17:17:46.305097Z",
     "iopub.status.busy": "2023-04-01T17:17:46.304830Z",
     "iopub.status.idle": "2023-04-01T17:17:46.329680Z",
     "shell.execute_reply": "2023-04-01T17:17:46.328951Z",
     "shell.execute_reply.started": "2023-04-01T17:17:46.305074Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Ball_1  Ball_2  Ball_3  Ball_4  Ball_5  Ball_6  Ball_Bonus\n",
      "Date                                                                  \n",
      "2016-01-02      12      25      29      40      48      56          52\n",
      "2016-01-06       8      30      40      50      54      57          13\n",
      "2016-01-09      26      27      46      47      52      58          48\n",
      "2016-01-13       4      12      38      46      57      59           8\n",
      "2016-01-16       1       8      12      25      43      52          38\n",
      "...            ...     ...     ...     ...     ...     ...         ...\n",
      "2020-08-08       2      40      49      53      56      58          41\n",
      "2020-08-12       6       8      27      31      37      51          20\n",
      "2020-08-15      10      14      16      22      25      34          21\n",
      "2020-08-19      21      22      32      36      45      50           5\n",
      "2020-08-22      19      23      24      28      37      51          12\n",
      "\n",
      "[485 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "lotto = pd.read_csv('../input/uk-lotto-draw-history-20162020/lotto_history.csv', index_col = 'Date')\n",
    "print(lotto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split\n",
    "Use the last 50 draws as the test dataset\n",
    "Use a sliding window of 10 to split the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-01T17:17:46.331001Z",
     "iopub.status.busy": "2023-04-01T17:17:46.330744Z",
     "iopub.status.idle": "2023-04-01T17:17:46.341365Z",
     "shell.execute_reply": "2023-04-01T17:17:46.340482Z",
     "shell.execute_reply.started": "2023-04-01T17:17:46.330978Z"
    }
   },
   "outputs": [],
   "source": [
    "data = lotto.values - 1\n",
    "train = data[:-50]\n",
    "test = data[-50:]\n",
    "\n",
    "w = 10\n",
    "X_train = []\n",
    "y_train = []\n",
    "for i in range(w, len(train)):\n",
    "    X_train.append(train[i - w: i, :])\n",
    "    y_train.append(train[i])\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n",
    "inputs = data[data.shape[0] - test.shape[0] - w:]\n",
    "X_test = []\n",
    "for i in range(w, inputs.shape[0]):\n",
    "    X_test.append(inputs[i - w: i, :])\n",
    "X_test = np.array(X_test)\n",
    "y_test = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-01T17:17:46.342822Z",
     "iopub.status.busy": "2023-04-01T17:17:46.342529Z",
     "iopub.status.idle": "2023-04-01T17:17:46.352109Z",
     "shell.execute_reply": "2023-04-01T17:17:46.351191Z",
     "shell.execute_reply.started": "2023-04-01T17:17:46.342796Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(485, 7)\n",
      "(425, 10, 7)\n",
      "(425, 7)\n",
      "(50, 10, 7)\n",
      "(50, 7)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq Model to Predict Future Draws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input the last 10 draws and sequentially predict the next draw.\n",
    "Monitor the performance by sparse categorical crossentropy and sparse top k categorical accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-01T17:17:46.353530Z",
     "iopub.status.busy": "2023-04-01T17:17:46.353228Z",
     "iopub.status.idle": "2023-04-01T17:17:46.361160Z",
     "shell.execute_reply": "2023-04-01T17:17:46.360354Z",
     "shell.execute_reply.started": "2023-04-01T17:17:46.353504Z"
    }
   },
   "outputs": [],
   "source": [
    "embed_dim = (59 // 2) + 1\n",
    "dropout_rate = 0.5\n",
    "spatial_dropout_rate = 0.5\n",
    "steps_before = w\n",
    "steps_after = 7\n",
    "feature_count = embed_dim * 7\n",
    "hidden_neurons = [64, 32] \n",
    "bidirectional = True \n",
    "attention_style = 'Bahdanau'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-01T17:17:46.362737Z",
     "iopub.status.busy": "2023-04-01T17:17:46.362441Z",
     "iopub.status.idle": "2023-04-01T17:17:48.170159Z",
     "shell.execute_reply": "2023-04-01T17:17:48.169141Z",
     "shell.execute_reply.started": "2023-04-01T17:17:46.362710Z"
    }
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    \n",
    "    inp0 = Input(shape = (w, X_train.shape[2]))\n",
    "    \n",
    "    # Embed 59 categories into a 30-dimension continuous-number vector for each ball\n",
    "    inp1 = Lambda(lambda x: x[:, :, 0])(inp0)\n",
    "    inp1 = Embedding(59, embed_dim)(inp1)\n",
    "    inp1 = SpatialDropout1D(spatial_dropout_rate)(inp1)\n",
    "    \n",
    "    inp2 = Lambda(lambda x: x[:, :, 1])(inp0)\n",
    "    inp2 = Embedding(59, embed_dim)(inp2)\n",
    "    inp2 = SpatialDropout1D(spatial_dropout_rate)(inp2)\n",
    "    \n",
    "    inp3 = Lambda(lambda x: x[:, :, 2])(inp0)\n",
    "    inp3 = Embedding(59, embed_dim)(inp3)\n",
    "    inp3 = SpatialDropout1D(spatial_dropout_rate)(inp3)\n",
    "    \n",
    "    inp4 = Lambda(lambda x: x[:, :, 3])(inp0)\n",
    "    inp4 = Embedding(59, embed_dim)(inp4)\n",
    "    inp4 = SpatialDropout1D(spatial_dropout_rate)(inp4)\n",
    "    \n",
    "    inp5 = Lambda(lambda x: x[:, :, 4])(inp0)\n",
    "    inp5 = Embedding(59, embed_dim)(inp5)\n",
    "    inp5 = SpatialDropout1D(spatial_dropout_rate)(inp5)    \n",
    "    \n",
    "    inp6 = Lambda(lambda x: x[:, :, 5])(inp0)\n",
    "    inp6 = Embedding(59, embed_dim)(inp6)\n",
    "    inp6 = SpatialDropout1D(spatial_dropout_rate)(inp6)\n",
    "    \n",
    "    inp7 = Lambda(lambda x: x[:, :, 6])(inp0)\n",
    "    inp7 = Embedding(59, embed_dim)(inp7)\n",
    "    inp7 = SpatialDropout1D(spatial_dropout_rate)(inp7)\n",
    "    \n",
    "    inp = Concatenate()([inp1, inp2, inp3, inp4, inp5, inp6, inp7])\n",
    "    \n",
    "    # Seq2Seq model with attention or bidirectional encoder\n",
    "    \n",
    "    num_layers = len(hidden_neurons)\n",
    "    \n",
    "    sh_list, h_list, c_list = [inp], [], []\n",
    "    \n",
    "    if bidirectional:\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "    \n",
    "            sh, fh, fc, bh, bc = Bidirectional(LSTM(hidden_neurons[i],\n",
    "                                                    dropout = dropout_rate, \n",
    "                                                    return_state = True, \n",
    "                                                    return_sequences = True))(sh_list[-1])\n",
    "        \n",
    "            h = Concatenate()([fh, bh])\n",
    "            c = Concatenate()([fc, bc]) \n",
    "\n",
    "            sh_list.append(sh)\n",
    "            h_list.append(h)\n",
    "            c_list.append(c)\n",
    "        \n",
    "    else:\n",
    "    \n",
    "        for i in range(num_layers):\n",
    "\n",
    "            sh, h, c = LSTM(hidden_neurons[i], \n",
    "                            dropout = dropout_rate,\n",
    "                            return_state = True, \n",
    "                            return_sequences = True)(sh_list[-1])\n",
    "\n",
    "            sh_list.append(sh)\n",
    "            h_list.append(h)\n",
    "            c_list.append(c)\n",
    "    \n",
    "    decoder = RepeatVector(steps_after)(h_list[-1])\n",
    "    \n",
    "    if bidirectional:\n",
    "        \n",
    "        decoder_hidden_neurons = [hn * 2 for hn in hidden_neurons]\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        decoder_hidden_neurons = hidden_neurons\n",
    "    \n",
    "    for i in range(num_layers):\n",
    "        \n",
    "        decoder = LSTM(decoder_hidden_neurons[i],\n",
    "                       dropout = dropout_rate, \n",
    "                       return_sequences = True)(decoder, initial_state = [h_list[i], c_list[i]])\n",
    "       \n",
    "    if attention_style == 'Bahdanau':\n",
    "        \n",
    "        context = AdditiveAttention(dropout = dropout_rate)([decoder, sh_list[-1]])\n",
    "        \n",
    "        decoder = concatenate([context, decoder])\n",
    "        \n",
    "    elif attention_style == 'Luong':\n",
    "        \n",
    "        context = Attention(dropout = dropout_rate)([decoder, sh_list[-1]])\n",
    "        \n",
    "        decoder = concatenate([context, decoder])\n",
    "    \n",
    "    out = Dense(59, activation = 'softmax')(decoder)\n",
    "\n",
    "    model = Model(inputs = inp0, outputs = out)\n",
    "    \n",
    "    sparse_top_k = tf.keras.metrics.SparseTopKCategoricalAccuracy(k = 5, name = 'sparse_top_k')\n",
    "\n",
    "    model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = [sparse_top_k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-01T17:17:48.171679Z",
     "iopub.status.busy": "2023-04-01T17:17:48.171370Z",
     "iopub.status.idle": "2023-04-01T17:17:48.254371Z",
     "shell.execute_reply": "2023-04-01T17:17:48.253341Z",
     "shell.execute_reply.started": "2023-04-01T17:17:48.171652Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 10, 7)]      0           []                               \n",
      "                                                                                                  \n",
      " lambda_7 (Lambda)              (None, 10)           0           ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lambda_8 (Lambda)              (None, 10)           0           ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lambda_9 (Lambda)              (None, 10)           0           ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lambda_10 (Lambda)             (None, 10)           0           ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lambda_11 (Lambda)             (None, 10)           0           ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lambda_12 (Lambda)             (None, 10)           0           ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lambda_13 (Lambda)             (None, 10)           0           ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_7 (Embedding)        (None, 10, 30)       1770        ['lambda_7[0][0]']               \n",
      "                                                                                                  \n",
      " embedding_8 (Embedding)        (None, 10, 30)       1770        ['lambda_8[0][0]']               \n",
      "                                                                                                  \n",
      " embedding_9 (Embedding)        (None, 10, 30)       1770        ['lambda_9[0][0]']               \n",
      "                                                                                                  \n",
      " embedding_10 (Embedding)       (None, 10, 30)       1770        ['lambda_10[0][0]']              \n",
      "                                                                                                  \n",
      " embedding_11 (Embedding)       (None, 10, 30)       1770        ['lambda_11[0][0]']              \n",
      "                                                                                                  \n",
      " embedding_12 (Embedding)       (None, 10, 30)       1770        ['lambda_12[0][0]']              \n",
      "                                                                                                  \n",
      " embedding_13 (Embedding)       (None, 10, 30)       1770        ['lambda_13[0][0]']              \n",
      "                                                                                                  \n",
      " spatial_dropout1d_7 (SpatialDr  (None, 10, 30)      0           ['embedding_7[0][0]']            \n",
      " opout1D)                                                                                         \n",
      "                                                                                                  \n",
      " spatial_dropout1d_8 (SpatialDr  (None, 10, 30)      0           ['embedding_8[0][0]']            \n",
      " opout1D)                                                                                         \n",
      "                                                                                                  \n",
      " spatial_dropout1d_9 (SpatialDr  (None, 10, 30)      0           ['embedding_9[0][0]']            \n",
      " opout1D)                                                                                         \n",
      "                                                                                                  \n",
      " spatial_dropout1d_10 (SpatialD  (None, 10, 30)      0           ['embedding_10[0][0]']           \n",
      " ropout1D)                                                                                        \n",
      "                                                                                                  \n",
      " spatial_dropout1d_11 (SpatialD  (None, 10, 30)      0           ['embedding_11[0][0]']           \n",
      " ropout1D)                                                                                        \n",
      "                                                                                                  \n",
      " spatial_dropout1d_12 (SpatialD  (None, 10, 30)      0           ['embedding_12[0][0]']           \n",
      " ropout1D)                                                                                        \n",
      "                                                                                                  \n",
      " spatial_dropout1d_13 (SpatialD  (None, 10, 30)      0           ['embedding_13[0][0]']           \n",
      " ropout1D)                                                                                        \n",
      "                                                                                                  \n",
      " concatenate_6 (Concatenate)    (None, 10, 210)      0           ['spatial_dropout1d_7[0][0]',    \n",
      "                                                                  'spatial_dropout1d_8[0][0]',    \n",
      "                                                                  'spatial_dropout1d_9[0][0]',    \n",
      "                                                                  'spatial_dropout1d_10[0][0]',   \n",
      "                                                                  'spatial_dropout1d_11[0][0]',   \n",
      "                                                                  'spatial_dropout1d_12[0][0]',   \n",
      "                                                                  'spatial_dropout1d_13[0][0]']   \n",
      "                                                                                                  \n",
      " bidirectional_2 (Bidirectional  [(None, 10, 128),   140800      ['concatenate_6[0][0]']          \n",
      " )                               (None, 64),                                                      \n",
      "                                 (None, 64),                                                      \n",
      "                                 (None, 64),                                                      \n",
      "                                 (None, 64)]                                                      \n",
      "                                                                                                  \n",
      " bidirectional_3 (Bidirectional  [(None, 10, 64),    41216       ['bidirectional_2[0][0]']        \n",
      " )                               (None, 32),                                                      \n",
      "                                 (None, 32),                                                      \n",
      "                                 (None, 32),                                                      \n",
      "                                 (None, 32)]                                                      \n",
      "                                                                                                  \n",
      " concatenate_9 (Concatenate)    (None, 64)           0           ['bidirectional_3[0][1]',        \n",
      "                                                                  'bidirectional_3[0][3]']        \n",
      "                                                                                                  \n",
      " repeat_vector_1 (RepeatVector)  (None, 7, 64)       0           ['concatenate_9[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_7 (Concatenate)    (None, 128)          0           ['bidirectional_2[0][1]',        \n",
      "                                                                  'bidirectional_2[0][3]']        \n",
      "                                                                                                  \n",
      " concatenate_8 (Concatenate)    (None, 128)          0           ['bidirectional_2[0][2]',        \n",
      "                                                                  'bidirectional_2[0][4]']        \n",
      "                                                                                                  \n",
      " lstm_6 (LSTM)                  (None, 7, 128)       98816       ['repeat_vector_1[0][0]',        \n",
      "                                                                  'concatenate_7[0][0]',          \n",
      "                                                                  'concatenate_8[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_10 (Concatenate)   (None, 64)           0           ['bidirectional_3[0][2]',        \n",
      "                                                                  'bidirectional_3[0][4]']        \n",
      "                                                                                                  \n",
      " lstm_7 (LSTM)                  (None, 7, 64)        49408       ['lstm_6[0][0]',                 \n",
      "                                                                  'concatenate_9[0][0]',          \n",
      "                                                                  'concatenate_10[0][0]']         \n",
      "                                                                                                  \n",
      " additive_attention_1 (Additive  (None, 7, 64)       64          ['lstm_7[0][0]',                 \n",
      " Attention)                                                       'bidirectional_3[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate_11 (Concatenate)   (None, 7, 128)       0           ['additive_attention_1[0][0]',   \n",
      "                                                                  'lstm_7[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 7, 59)        7611        ['concatenate_11[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 350,305\n",
      "Trainable params: 350,305\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-01T17:17:48.255969Z",
     "iopub.status.busy": "2023-04-01T17:17:48.255679Z",
     "iopub.status.idle": "2023-04-01T17:17:48.261096Z",
     "shell.execute_reply": "2023-04-01T17:17:48.260181Z",
     "shell.execute_reply.started": "2023-04-01T17:17:48.255943Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "plot_model(model, show_shapes = True, show_layer_names = True, rankdir = 'TB', dpi = 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-01T17:17:48.262512Z",
     "iopub.status.busy": "2023-04-01T17:17:48.262214Z",
     "iopub.status.idle": "2023-04-01T17:17:48.272450Z",
     "shell.execute_reply": "2023-04-01T17:17:48.271561Z",
     "shell.execute_reply.started": "2023-04-01T17:17:48.262485Z"
    }
   },
   "outputs": [],
   "source": [
    "class CosineAnnealingScheduler(callbacks.Callback):\n",
    "    \"\"\"Cosine annealing scheduler.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, T_max, eta_max, eta_min = 0, verbose = 0):\n",
    "        super(CosineAnnealingScheduler, self).__init__()\n",
    "        self.T_max = T_max\n",
    "        self.eta_max = eta_max\n",
    "        self.eta_min = eta_min\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs = None):\n",
    "        if not hasattr(self.model.optimizer, 'lr'):\n",
    "            raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
    "        lr = self.eta_min + (self.eta_max - self.eta_min) * (1 + math.cos(math.pi * epoch / self.T_max)) / 2\n",
    "        backend.set_value(self.model.optimizer.lr, lr)\n",
    "        if self.verbose > 0:\n",
    "            print('\\nEpoch %05d: CosineAnnealingScheduler setting learning '\n",
    "                  'rate to %s.' % (epoch + 1, lr))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs = None):\n",
    "        logs = logs or {}\n",
    "        logs['lr'] = backend.get_value(self.model.optimizer.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-04-01T17:17:48.274457Z",
     "iopub.status.busy": "2023-04-01T17:17:48.273621Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "14/14 - 16s - loss: 4.0777 - sparse_top_k: 0.0847 - val_loss: 4.0776 - val_sparse_top_k: 0.0829 - lr: 1.0000e-04 - 16s/epoch - 1s/step\n",
      "Epoch 2/200\n",
      "14/14 - 1s - loss: 4.0767 - sparse_top_k: 0.0945 - val_loss: 4.0774 - val_sparse_top_k: 0.0886 - lr: 9.9994e-05 - 611ms/epoch - 44ms/step\n",
      "Epoch 3/200\n",
      "14/14 - 1s - loss: 4.0764 - sparse_top_k: 0.0975 - val_loss: 4.0772 - val_sparse_top_k: 0.1000 - lr: 9.9978e-05 - 589ms/epoch - 42ms/step\n",
      "Epoch 4/200\n",
      "14/14 - 0s - loss: 4.0761 - sparse_top_k: 0.1059 - val_loss: 4.0769 - val_sparse_top_k: 0.0971 - lr: 9.9950e-05 - 479ms/epoch - 34ms/step\n",
      "Epoch 5/200\n",
      "14/14 - 1s - loss: 4.0749 - sparse_top_k: 0.1227 - val_loss: 4.0766 - val_sparse_top_k: 0.0943 - lr: 9.9911e-05 - 500ms/epoch - 36ms/step\n",
      "Epoch 6/200\n",
      "14/14 - 0s - loss: 4.0741 - sparse_top_k: 0.1139 - val_loss: 4.0762 - val_sparse_top_k: 0.0857 - lr: 9.9861e-05 - 498ms/epoch - 36ms/step\n",
      "Epoch 7/200\n",
      "14/14 - 0s - loss: 4.0734 - sparse_top_k: 0.1220 - val_loss: 4.0757 - val_sparse_top_k: 0.0914 - lr: 9.9800e-05 - 480ms/epoch - 34ms/step\n",
      "Epoch 8/200\n",
      "14/14 - 0s - loss: 4.0719 - sparse_top_k: 0.1304 - val_loss: 4.0749 - val_sparse_top_k: 0.1000 - lr: 9.9728e-05 - 475ms/epoch - 34ms/step\n",
      "Epoch 9/200\n",
      "14/14 - 0s - loss: 4.0707 - sparse_top_k: 0.1318 - val_loss: 4.0737 - val_sparse_top_k: 0.0943 - lr: 9.9645e-05 - 480ms/epoch - 34ms/step\n",
      "Epoch 10/200\n",
      "14/14 - 0s - loss: 4.0685 - sparse_top_k: 0.1220 - val_loss: 4.0719 - val_sparse_top_k: 0.0857 - lr: 9.9551e-05 - 475ms/epoch - 34ms/step\n",
      "Epoch 11/200\n",
      "14/14 - 0s - loss: 4.0643 - sparse_top_k: 0.1355 - val_loss: 4.0688 - val_sparse_top_k: 0.0829 - lr: 9.9446e-05 - 470ms/epoch - 34ms/step\n",
      "Epoch 12/200\n",
      "14/14 - 0s - loss: 4.0578 - sparse_top_k: 0.1439 - val_loss: 4.0629 - val_sparse_top_k: 0.0886 - lr: 9.9330e-05 - 476ms/epoch - 34ms/step\n",
      "Epoch 13/200\n",
      "14/14 - 0s - loss: 4.0439 - sparse_top_k: 0.1378 - val_loss: 4.0516 - val_sparse_top_k: 0.0800 - lr: 9.9203e-05 - 477ms/epoch - 34ms/step\n",
      "Epoch 14/200\n",
      "14/14 - 0s - loss: 4.0180 - sparse_top_k: 0.1284 - val_loss: 4.0426 - val_sparse_top_k: 0.0771 - lr: 9.9065e-05 - 478ms/epoch - 34ms/step\n",
      "Epoch 15/200\n",
      "14/14 - 1s - loss: 3.9945 - sparse_top_k: 0.1392 - val_loss: 4.0394 - val_sparse_top_k: 0.0943 - lr: 9.8916e-05 - 504ms/epoch - 36ms/step\n",
      "Epoch 16/200\n",
      "14/14 - 1s - loss: 3.9751 - sparse_top_k: 0.1529 - val_loss: 4.0188 - val_sparse_top_k: 0.1143 - lr: 9.8757e-05 - 573ms/epoch - 41ms/step\n",
      "Epoch 17/200\n",
      "14/14 - 1s - loss: 3.9475 - sparse_top_k: 0.1748 - val_loss: 3.9887 - val_sparse_top_k: 0.1371 - lr: 9.8586e-05 - 562ms/epoch - 40ms/step\n",
      "Epoch 18/200\n",
      "14/14 - 1s - loss: 3.9116 - sparse_top_k: 0.1903 - val_loss: 3.9502 - val_sparse_top_k: 0.1600 - lr: 9.8405e-05 - 557ms/epoch - 40ms/step\n",
      "Epoch 19/200\n",
      "14/14 - 1s - loss: 3.8804 - sparse_top_k: 0.1909 - val_loss: 3.9185 - val_sparse_top_k: 0.1629 - lr: 9.8213e-05 - 556ms/epoch - 40ms/step\n",
      "Epoch 20/200\n",
      "14/14 - 0s - loss: 3.8580 - sparse_top_k: 0.1963 - val_loss: 3.8963 - val_sparse_top_k: 0.1629 - lr: 9.8011e-05 - 474ms/epoch - 34ms/step\n",
      "Epoch 21/200\n",
      "14/14 - 0s - loss: 3.8313 - sparse_top_k: 0.2061 - val_loss: 3.8753 - val_sparse_top_k: 0.1629 - lr: 9.7798e-05 - 465ms/epoch - 33ms/step\n",
      "Epoch 22/200\n",
      "14/14 - 0s - loss: 3.8149 - sparse_top_k: 0.2114 - val_loss: 3.8630 - val_sparse_top_k: 0.1629 - lr: 9.7574e-05 - 473ms/epoch - 34ms/step\n",
      "Epoch 23/200\n",
      "14/14 - 1s - loss: 3.7998 - sparse_top_k: 0.2114 - val_loss: 3.8500 - val_sparse_top_k: 0.1686 - lr: 9.7340e-05 - 557ms/epoch - 40ms/step\n",
      "Epoch 24/200\n",
      "14/14 - 1s - loss: 3.7931 - sparse_top_k: 0.2161 - val_loss: 3.8405 - val_sparse_top_k: 0.1829 - lr: 9.7095e-05 - 552ms/epoch - 39ms/step\n",
      "Epoch 25/200\n",
      "14/14 - 1s - loss: 3.7802 - sparse_top_k: 0.2245 - val_loss: 3.8311 - val_sparse_top_k: 0.2000 - lr: 9.6840e-05 - 545ms/epoch - 39ms/step\n",
      "Epoch 26/200\n",
      "14/14 - 0s - loss: 3.7704 - sparse_top_k: 0.2165 - val_loss: 3.8244 - val_sparse_top_k: 0.1971 - lr: 9.6575e-05 - 473ms/epoch - 34ms/step\n",
      "Epoch 27/200\n",
      "14/14 - 0s - loss: 3.7630 - sparse_top_k: 0.2171 - val_loss: 3.8178 - val_sparse_top_k: 0.1971 - lr: 9.6299e-05 - 487ms/epoch - 35ms/step\n",
      "Epoch 28/200\n",
      "14/14 - 1s - loss: 3.7549 - sparse_top_k: 0.2266 - val_loss: 3.8160 - val_sparse_top_k: 0.1857 - lr: 9.6013e-05 - 516ms/epoch - 37ms/step\n",
      "Epoch 29/200\n",
      "14/14 - 1s - loss: 3.7482 - sparse_top_k: 0.2376 - val_loss: 3.8072 - val_sparse_top_k: 0.1857 - lr: 9.5717e-05 - 501ms/epoch - 36ms/step\n",
      "Epoch 30/200\n",
      "14/14 - 0s - loss: 3.7434 - sparse_top_k: 0.2266 - val_loss: 3.8031 - val_sparse_top_k: 0.1914 - lr: 9.5411e-05 - 497ms/epoch - 36ms/step\n",
      "Epoch 31/200\n",
      "14/14 - 0s - loss: 3.7288 - sparse_top_k: 0.2383 - val_loss: 3.7982 - val_sparse_top_k: 0.2000 - lr: 9.5095e-05 - 477ms/epoch - 34ms/step\n",
      "Epoch 32/200\n",
      "14/14 - 0s - loss: 3.7320 - sparse_top_k: 0.2296 - val_loss: 3.7969 - val_sparse_top_k: 0.1886 - lr: 9.4769e-05 - 468ms/epoch - 33ms/step\n",
      "Epoch 33/200\n",
      "14/14 - 0s - loss: 3.7266 - sparse_top_k: 0.2249 - val_loss: 3.7927 - val_sparse_top_k: 0.1943 - lr: 9.4434e-05 - 466ms/epoch - 33ms/step\n",
      "Epoch 34/200\n",
      "14/14 - 0s - loss: 3.7166 - sparse_top_k: 0.2343 - val_loss: 3.7868 - val_sparse_top_k: 0.1943 - lr: 9.4088e-05 - 470ms/epoch - 34ms/step\n",
      "Epoch 35/200\n",
      "14/14 - 0s - loss: 3.7120 - sparse_top_k: 0.2387 - val_loss: 3.7855 - val_sparse_top_k: 0.1886 - lr: 9.3733e-05 - 472ms/epoch - 34ms/step\n",
      "Epoch 36/200\n",
      "14/14 - 0s - loss: 3.7074 - sparse_top_k: 0.2403 - val_loss: 3.7809 - val_sparse_top_k: 0.2000 - lr: 9.3369e-05 - 467ms/epoch - 33ms/step\n",
      "Epoch 37/200\n",
      "14/14 - 0s - loss: 3.7070 - sparse_top_k: 0.2370 - val_loss: 3.7815 - val_sparse_top_k: 0.2000 - lr: 9.2995e-05 - 468ms/epoch - 33ms/step\n",
      "Epoch 38/200\n",
      "14/14 - 0s - loss: 3.7047 - sparse_top_k: 0.2464 - val_loss: 3.7776 - val_sparse_top_k: 0.2000 - lr: 9.2611e-05 - 468ms/epoch - 33ms/step\n",
      "Epoch 39/200\n",
      "14/14 - 0s - loss: 3.6866 - sparse_top_k: 0.2437 - val_loss: 3.7734 - val_sparse_top_k: 0.1971 - lr: 9.2219e-05 - 464ms/epoch - 33ms/step\n",
      "Epoch 40/200\n",
      "14/14 - 0s - loss: 3.6950 - sparse_top_k: 0.2497 - val_loss: 3.7691 - val_sparse_top_k: 0.2000 - lr: 9.1817e-05 - 466ms/epoch - 33ms/step\n",
      "Epoch 41/200\n",
      "14/14 - 0s - loss: 3.6911 - sparse_top_k: 0.2491 - val_loss: 3.7705 - val_sparse_top_k: 0.2000 - lr: 9.1406e-05 - 483ms/epoch - 35ms/step\n",
      "Epoch 42/200\n",
      "14/14 - 1s - loss: 3.6872 - sparse_top_k: 0.2491 - val_loss: 3.7657 - val_sparse_top_k: 0.2057 - lr: 9.0986e-05 - 585ms/epoch - 42ms/step\n",
      "Epoch 43/200\n",
      "14/14 - 0s - loss: 3.6774 - sparse_top_k: 0.2545 - val_loss: 3.7643 - val_sparse_top_k: 0.2000 - lr: 9.0557e-05 - 497ms/epoch - 36ms/step\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 200\n",
    "BATCH_SIZE = 32\n",
    "LR_MAX = 1e-4\n",
    "LR_MIN = 1e-5\n",
    "\n",
    "cas = CosineAnnealingScheduler(EPOCHS, LR_MAX, LR_MIN)\n",
    "\n",
    "ckp = callbacks.ModelCheckpoint('best_model.hdf5', monitor = 'val_sparse_top_k', verbose = 0, \n",
    "                                save_best_only = True, save_weights_only = False, mode = 'max')\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "                    validation_data = (X_test, y_test), \n",
    "                    callbacks = [ckp, cas], \n",
    "                    epochs = EPOCHS, \n",
    "                    batch_size = BATCH_SIZE, \n",
    "                    verbose = 2)\n",
    "\n",
    "hist = pd.DataFrame(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hist['val_sparse_top_k'].max())\n",
    "\n",
    "plt.figure(figsize = (8, 6))\n",
    "plt.semilogy(hist['sparse_top_k'], '-r', label = 'Training')\n",
    "plt.semilogy(hist['val_sparse_top_k'], '-b', label = 'Validation')\n",
    "plt.ylabel('Sparse Top K Accuracy', fontsize = 14)\n",
    "plt.xlabel('Epochs', fontsize = 14)\n",
    "plt.legend(fontsize = 14)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('best_model.hdf5')\n",
    "pred = model.predict(X_test)\n",
    "pred = np.argmax(pred, axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(y_test.shape[0]):\n",
    "    print('Prediction:\\t', pred[i] + 1)\n",
    "    print('GoundTruth:\\t', y_test[i] + 1)\n",
    "    print('-' * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the Future Draw on 2020/Aug/26\n",
    "The Beam Search method is use to output 10 possible draws.\n",
    "It is worth noting that Replace = False means no duplicate balls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_latest = X_test[-1][1:]\n",
    "X_latest = np.concatenate([X_latest, y_test[-1].reshape(1, 7)], axis = 0)\n",
    "X_latest = X_latest.reshape(1, X_latest.shape[0], X_latest.shape[1])\n",
    "print(X_latest + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beam search\n",
    "def beam_search_decoder(data, k, replace = True):\n",
    "    sequences = [[list(), 0.0]]\n",
    "    # walk over each step in sequence\n",
    "    for row in data:\n",
    "        all_candidates = list()\n",
    "        # expand each current candidate\n",
    "        for i in range(len(sequences)):\n",
    "            seq, score = sequences[i]\n",
    "            best_k = np.argsort(row)[-k:]\n",
    "            for j in best_k:\n",
    "                candidate = [seq + [j], score + math.log(row[j])]\n",
    "                if replace:\n",
    "                    all_candidates.append(candidate)\n",
    "                elif (replace == False) and (len(set(candidate[0])) == len(candidate[0])):\n",
    "                    all_candidates.append(candidate)\n",
    "        # order all candidates by score\n",
    "        ordered = sorted(all_candidates, key = lambda tup:tup[1], reverse = True)\n",
    "        # select k best\n",
    "        sequences = ordered[:k]\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_latest = model.predict(X_latest)\n",
    "pred_latest = np.squeeze(pred_latest)\n",
    "pred_latest_greedy = np.argmax(pred_latest, axis = 1)\n",
    "print(pred_latest_greedy + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_width = 10\n",
    "replace = True\n",
    "\n",
    "result = beam_search_decoder(pred_latest, beam_width, replace)\n",
    "print('Beam Width:\\t', beam_width)\n",
    "print('Replace:\\t', replace)\n",
    "print('-' * 85)\n",
    "for seq in result:\n",
    "    print('Prediction: ', np.array(seq[0]) + 1, '\\tLog Likelihood: ', seq[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_width = 10\n",
    "replace = False\n",
    "\n",
    "result = beam_search_decoder(pred_latest, beam_width, replace)\n",
    "print('Beam Width:\\t', beam_width)\n",
    "print('Replace:\\t', replace)\n",
    "print('-' * 85)\n",
    "for seq in result:\n",
    "    print('Prediction: ', np.array(seq[0]) + 1, '\\tLog Likelihood: ', seq[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
